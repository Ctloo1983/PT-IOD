{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MtJ1pm6CP5XF"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eVUunvJoP5XI"
   },
   "source": [
    "# Lab 3.2.3 \n",
    "# *Mining Social Media on Reddit*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-kiLThkbP5XK"
   },
   "source": [
    "## The Reddit API and the PRAW Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "98HzUiRNP5XN"
   },
   "source": [
    "The Reddit API is rich and complex, with many endpoints (https://www.reddit.com/dev/api/). It includes methods for navigating its collections, which include various kinds of media as well as comments. Fortunately, the Python library PRAW reduces much of this complexity.\n",
    "\n",
    "Reddit requires developers to create and authenticate an app before they can use the API, but the process is much less onerus than some, and does not have waiting period for approval of new developers (as of 18 August 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wx3UQZPWP5XP"
   },
   "source": [
    "### 1. Create a Reddit App\n",
    "\n",
    "Go to https://www.reddit.com/prefs/apps and click \"create an app\".\n",
    "\n",
    "Enter the following in the form:\n",
    "\n",
    "- a name for your app\n",
    "- select \"script\" radio button\n",
    "- a description\n",
    "- a redirect URI\n",
    "\n",
    "(Nb. For pulling data into a data science experiment, a local port can be used for the Redirect URI; try http://127.0.0.1:1410)\n",
    "\n",
    "- click \"create app\"\n",
    "- from the form that displays, copy the following to a local text file (or to this notebook):\n",
    "\n",
    "  - name (the name you gave to your app)\n",
    "  - redirect URI\n",
    "  - personal use script (this is your OAuth 2 Client ID)\n",
    "  - secret (this is your OAuth 2 Secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oNdLwIPIP5XR"
   },
   "source": [
    "### 2. Register for API Access\n",
    "\n",
    "- follow the link at https://www.reddit.com/wiki/api and read the terms of use for Reddit API access \n",
    "- fill in the form fields at the bottom \n",
    "  - make sure to enter your new OAuth Client ID where indicated\n",
    "  - your use case could be something like \"Training in API usage for data science projects\"\n",
    "  - your platform could be something like \"Jupyter Notebooks / Python\"\n",
    "  \n",
    "- click \"SUBMIT\"\n",
    " \n",
    "- when asked for User-Agent, enter something that fits this pattern:\n",
    "  `your_os-python:your_reddit_appname:v1.0 (by /u/your_reddit_username)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N22ya_vtP5XT"
   },
   "source": [
    "### 3. Load Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okips5jtP5XV"
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import requests\n",
    "import json\n",
    "import pprint\n",
    "from datetime import datetime, date, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mAMBDYOOP5Xc"
   },
   "source": [
    "### 4. Authenticate from your Python script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b5fMVZ_DP5Xd"
   },
   "source": [
    "You could assign your authentication details explicitly, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RARMbQ0-P5Xf"
   },
   "outputs": [],
   "source": [
    "my_user_agent = 'macOS-python:matsalleh2020:v1.0 (by /u/matsalleh2020)'   # your user Agent string goes in here\n",
    "my_client_id = ' '   # your Client ID string goes in here\n",
    "my_client_secret = ' '   # your Secret string goes in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "396vbxD6P5Xl"
   },
   "source": [
    "A better way would be to store these details externally, so they are not displayed in the notebook:\n",
    "\n",
    "- create a file called \"auth_reddit.json\" in your \"notebooks\" directory, and save your credentials there in JSON format:\n",
    "\n",
    "`{   \"my_client_id\": \"your Client ID string goes in here\",` <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;` \"my_client_secret\": \"your Secret string goes in here\",` <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`\"my_user_agent\": \"your user Agent string goes in here\"` <br>\n",
    "`}`\n",
    "\n",
    "Use the following code to load the credentials:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KtkU50P7P5Xn",
    "outputId": "18502589-7a0b-46f1-fd31-89dc88965854"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gregory_murray/Documents/Magic Briefcase/Data Science/DG-SG-FT-16Apr21/Module 3/Answers'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()  # make sure your working directory is where the file is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oHTwqfvvP5Xx"
   },
   "outputs": [],
   "source": [
    "path_auth = 'auth_reddit.json'\n",
    "auth = json.loads(open(path_auth).read())\n",
    "\n",
    "# For debugging only:\n",
    "# pp = pprint.PrettyPrinter(indent=4)\n",
    "#pp.pprint(auth)\n",
    "\n",
    "my_user_agent = auth['my_user_agent']\n",
    "my_client_id = auth['my_client_id']\n",
    "my_client_secret = auth['my_client_secret']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nTnIrybxP5Xz"
   },
   "source": [
    "Security considerations: \n",
    "- this method only keeps your credentials invisible as long as nobody else gets access to this notebook file \n",
    "- if you wanted another user to have access to the executable notebook without divulging your credentials you should set up an OAuth 2.0 workflow to let them obtain and apply their own API tokens when using your app\n",
    "- if you just want to share your analyses, you could use a separate script (which you don't share) to fetch the data and save it locally, then use a second notebook (with no API access) to load and analyse the locally stored data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "upgA0mzrP5Xz"
   },
   "source": [
    "### 5. Exploring the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DItfqhBoP5X0"
   },
   "source": [
    "Here is how to connect to Reddit with read-only access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O35sEDJJP5X1",
    "outputId": "7eb21579-8c18-4ec5-de46-08fe5f279009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read-only = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.1.0 of praw is outdated. Version 7.2.0 was released Wednesday February 24, 2021.\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id = my_client_id, \n",
    "                     client_secret = my_client_secret, \n",
    "                     user_agent = my_user_agent)\n",
    "\n",
    "print('Read-only = ' + str(reddit.read_only))  # Output: True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0i84iF99P5X5"
   },
   "source": [
    "In the next cell, put the cursor after the '.' and hit the [tab] key to see the available members and methods in the response object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80IK0dE6P5X7"
   },
   "source": [
    "Consult the PRAW and Reddit API documentation. Print a few of the response members below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit.auth\n",
    "# reddit.comment\n",
    "# reddit.config\n",
    "# reddit.delete\n",
    "# reddit.domain\n",
    "# reddit.front\n",
    "# reddit.get\n",
    "# reddit.inbox\n",
    "# reddit.live\n",
    "# reddit.multireddit\n",
    "# reddit.patch\n",
    "# reddit.post\n",
    "# reddit.put\n",
    "# reddit.random_subreddit\n",
    "# reddit.read_only\n",
    "# reddit.redditor\n",
    "# reddit.redditors\n",
    "# reddit.submission\n",
    "# reddit.subreddit\n",
    "# reddit.subreddits\n",
    "# reddit.update_checked\n",
    "# reddit.user\n",
    "# reddit.validate_on_submit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0quBGKHrP5X9"
   },
   "source": [
    "Content in Reddit is grouped by topics called \"subreddits\". Content, called \"submissions\", is fetched by calling the `subreddit` method of the connection object (which is our `reddit` variable) with an argument that matches an actual topic. \n",
    "\n",
    "We also need to append a further method call to a \"subinstance\", such as one of the following:\n",
    "\n",
    "- controversial\n",
    "- gilded\n",
    "- hot\n",
    "- new\n",
    "- rising\n",
    "- top\n",
    "\n",
    "One of the submission objects members is `title`. Fetch and print 10 submission titles from the 'learnpython' subreddit using one of the subinstances above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zciv6cSNP5X9",
    "outputId": "335cd9cd-7db0-49ef-a40f-50c87943beba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask Anything Monday - Weekly Thread\n",
      "Sharing my win.\n",
      "Python 101: 2nd Edition is FREE for PyCon 2021!\n",
      "Thanks for the help\n",
      "Flask Showing Wrong URL\n",
      "Scraping Amazon with requests + BeautifulSoup\n",
      "Why is Python solving this equation incorrectly?\n",
      "How long does it take to learn Python?\n",
      "how to check for cropped copies of images with python\n",
      "Code works as expected in VSCode debugger, but terminal closes socket early\n"
     ]
    }
   ],
   "source": [
    "for submission in reddit.subreddit('learnpython').hot(limit=10):\n",
    "    print(submission.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kClUAtmfP5X_"
   },
   "source": [
    "Now retrieve 10 authors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NJJbYFppP5YA",
    "outputId": "37815d5e-41ee-4f16-873b-c76bc52bbec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoModerator\n",
      "Brothercford\n",
      "driscollis\n",
      "DanDannymite_27\n",
      "beefyliltank\n",
      "Nicolozz0\n",
      "TheRealTengri\n",
      "Ascetic_Banana-20\n",
      "daddygoose04\n",
      "CharmingMidnight8191\n"
     ]
    }
   ],
   "source": [
    "for submission in reddit.subreddit('learnpython').hot(limit=10):\n",
    "    print(submission.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "voyL9o67P5YD"
   },
   "source": [
    "Note that we obtained the titles and authors from separate API calls. Can we expect these to correspond to the same submissions? If not, how could we gurantee that they do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c2LTSdo5P5YE",
    "outputId": "b5cde4a9-96f1-4b1d-e2dc-e4525ba50346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: AutoModerator | Title: Ask Anything Monday - Weekly Thread\n",
      "Author: Brothercford | Title: Sharing my win.\n",
      "Author: driscollis | Title: Python 101: 2nd Edition is FREE for PyCon 2021!\n",
      "Author: DanDannymite_27 | Title: Thanks for the help\n",
      "Author: beefyliltank | Title: Flask Showing Wrong URL\n",
      "Author: Nicolozz0 | Title: Scraping Amazon with requests + BeautifulSoup\n",
      "Author: a0311tr | Title: Writing Tests\n",
      "Author: ThePerceptionist | Title: Github Pages site that updates daily with content from Python script\n",
      "Author: johannadambergk | Title: Extracting numbers\n",
      "Author: TheRealTengri | Title: Why is Python solving this equation incorrectly?\n"
     ]
    }
   ],
   "source": [
    "#ANSWER:\n",
    "submissions = reddit.subreddit('learnpython').hot(limit=10)\n",
    "for submission in submissions:\n",
    "    print(\"Author: {} | Title: {}\".format(submission.author, submission.title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS : Various features can be extracted to a DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "posts = []\n",
    "ml_subreddit = reddit.subreddit('MachineLearning')\n",
    "for post in ml_subreddit.hot(limit=10):\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[D] Machine Learning - WAYR (What Are You Read...</td>\n",
       "      <td>14</td>\n",
       "      <td>n8m6ds</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>1</td>\n",
       "      <td>This is a place to share machine learning rese...</td>\n",
       "      <td>1.620619e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[R] Enhancing Photorealism Enhancement</td>\n",
       "      <td>34</td>\n",
       "      <td>nbyrcj</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://arxiv.org/abs/2105.04619</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>1.620990e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[D] Are ResNets as good as it gets?</td>\n",
       "      <td>193</td>\n",
       "      <td>nbgb6a</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>36</td>\n",
       "      <td>TLDR: For training from scratch on non-classi...</td>\n",
       "      <td>1.620940e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[R] Unsupervised Progressive Learning and the ...</td>\n",
       "      <td>6</td>\n",
       "      <td>nbzv92</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>0</td>\n",
       "      <td>Check out our recent work, which was accepted ...</td>\n",
       "      <td>1.620994e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[D] Disentangling Medical Image features using...</td>\n",
       "      <td>13</td>\n",
       "      <td>nbsp3t</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>1</td>\n",
       "      <td>Recently I found out about NFs and their prope...</td>\n",
       "      <td>1.620972e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[D] GAN training - aren't the double discrimin...</td>\n",
       "      <td>39</td>\n",
       "      <td>nbk34d</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>11</td>\n",
       "      <td>I've noticed that a lot of GAN codebases runs ...</td>\n",
       "      <td>1.620951e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[P] Twitter bot that tweets trending ML papers</td>\n",
       "      <td>45</td>\n",
       "      <td>nbioy3</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>7</td>\n",
       "      <td>Hey everyone!\\n\\nI created a twitter bot that ...</td>\n",
       "      <td>1.620947e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[D] Why is the baseline for fairness and bias ...</td>\n",
       "      <td>40</td>\n",
       "      <td>nbisqb</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>39</td>\n",
       "      <td>When we talk about ML models' accuracy and oth...</td>\n",
       "      <td>1.620947e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[P] Enigma: GPT-2 trained on 10K Nature Papers...</td>\n",
       "      <td>160</td>\n",
       "      <td>nb9ifz</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>31</td>\n",
       "      <td>Project Enigma: https://stefanzukin.com/enigma...</td>\n",
       "      <td>1.620913e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[P] Machine Learning in Physics?</td>\n",
       "      <td>57</td>\n",
       "      <td>nbdoc6</td>\n",
       "      <td>MachineLearning</td>\n",
       "      <td>https://www.reddit.com/r/MachineLearning/comme...</td>\n",
       "      <td>19</td>\n",
       "      <td>Hey everyone!\\n\\nI've been doing machine learn...</td>\n",
       "      <td>1.620931e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score      id  \\\n",
       "0  [D] Machine Learning - WAYR (What Are You Read...     14  n8m6ds   \n",
       "1             [R] Enhancing Photorealism Enhancement     34  nbyrcj   \n",
       "2                [D] Are ResNets as good as it gets?    193  nbgb6a   \n",
       "3  [R] Unsupervised Progressive Learning and the ...      6  nbzv92   \n",
       "4  [D] Disentangling Medical Image features using...     13  nbsp3t   \n",
       "5  [D] GAN training - aren't the double discrimin...     39  nbk34d   \n",
       "6     [P] Twitter bot that tweets trending ML papers     45  nbioy3   \n",
       "7  [D] Why is the baseline for fairness and bias ...     40  nbisqb   \n",
       "8  [P] Enigma: GPT-2 trained on 10K Nature Papers...    160  nb9ifz   \n",
       "9                   [P] Machine Learning in Physics?     57  nbdoc6   \n",
       "\n",
       "         subreddit                                                url  \\\n",
       "0  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "1  MachineLearning                   https://arxiv.org/abs/2105.04619   \n",
       "2  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "3  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "4  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "5  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "6  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "7  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "8  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "9  MachineLearning  https://www.reddit.com/r/MachineLearning/comme...   \n",
       "\n",
       "   num_comments                                               body  \\\n",
       "0             1  This is a place to share machine learning rese...   \n",
       "1             7                                                      \n",
       "2            36   TLDR: For training from scratch on non-classi...   \n",
       "3             0  Check out our recent work, which was accepted ...   \n",
       "4             1  Recently I found out about NFs and their prope...   \n",
       "5            11  I've noticed that a lot of GAN codebases runs ...   \n",
       "6             7  Hey everyone!\\n\\nI created a twitter bot that ...   \n",
       "7            39  When we talk about ML models' accuracy and oth...   \n",
       "8            31  Project Enigma: https://stefanzukin.com/enigma...   \n",
       "9            19  Hey everyone!\\n\\nI've been doing machine learn...   \n",
       "\n",
       "        created  \n",
       "0  1.620619e+09  \n",
       "1  1.620990e+09  \n",
       "2  1.620940e+09  \n",
       "3  1.620994e+09  \n",
       "4  1.620972e+09  \n",
       "5  1.620951e+09  \n",
       "6  1.620947e+09  \n",
       "7  1.620947e+09  \n",
       "8  1.620913e+09  \n",
       "9  1.620931e+09  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General information about the subreddit can be obtained and using the .description function on\n",
    "# the subreddit object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**[Rules For Posts](https://www.reddit.com/r/MachineLearning/about/rules/)**\n",
      "--------\n",
      "+[Research](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AResearch)\n",
      "--------\n",
      "+[Discussion](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ADiscussion)\n",
      "--------\n",
      "+[Project](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AProject)\n",
      "--------\n",
      "+[News](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ANews)\n",
      "--------\n",
      "***[@slashML on Twitter](https://twitter.com/slashML)***\n",
      "--------\n",
      "***[Chat with us on Slack](https://join.slack.com/t/rml-talk/shared_invite/enQtNjkyMzI3NjA2NTY2LWY0ZmRjZjNhYjI5NzYwM2Y0YzZhZWNiODQ3ZGFjYmI2NTU3YjE1ZDU5MzM2ZTQ4ZGJmOTFmNWVkMzFiMzVhYjg)***\n",
      "--------\n",
      "**Beginners:**\n",
      "--------\n",
      "Please have a look at [our FAQ and Link-Collection](http://www.reddit.com/r/MachineLearning/wiki/index)\n",
      "\n",
      "[Metacademy](http://www.metacademy.org) is a great resource which compiles lesson plans on popular machine learning topics.\n",
      "\n",
      "For Beginner questions please try /r/LearnMachineLearning , /r/MLQuestions or http://stackoverflow.com/\n",
      "\n",
      "For career related questions, visit /r/cscareerquestions/\n",
      "\n",
      "--------\n",
      "\n",
      "[Advanced Courses (2016)](https://www.reddit.com/r/MachineLearning/comments/51qhc8/phdlevel_courses?st=isz2lqdk&sh=56c58cd6)\n",
      "\n",
      "[Advanced Courses (2020)](https://www.reddit.com/r/MachineLearning/comments/fdw0ax/d_advanced_courses_update/)\n",
      "\n",
      "--------\n",
      "**AMAs:**\n",
      "\n",
      "[Pluribus Poker AI Team 7/19/2019](https://www.reddit.com/r/MachineLearning/comments/ceece3/ama_we_are_noam_brown_and_tuomas_sandholm/)\n",
      "\n",
      "[DeepMind AlphaStar team (1/24//2019)](https://www.reddit.com/r/MachineLearning/comments/ajgzoc/we_are_oriol_vinyals_and_david_silver_from/)\n",
      "\n",
      "[Libratus Poker AI Team (12/18/2017)]\n",
      "(https://www.reddit.com/r/MachineLearning/comments/7jn12v/ama_we_are_noam_brown_and_professor_tuomas/)\n",
      "\n",
      "[DeepMind AlphaGo Team (10/19/2017)](https://www.reddit.com/r/MachineLearning/comments/76xjb5/ama_we_are_david_silver_and_julian_schrittwieser/)\n",
      "\n",
      "[Google Brain Team (9/17/2017)](https://www.reddit.com/r/MachineLearning/comments/6z51xb/we_are_the_google_brain_team_wed_love_to_answer/)\n",
      "\n",
      "[Google Brain Team (8/11/2016)]\n",
      "(https://www.reddit.com/r/MachineLearning/comments/4w6tsv/ama_we_are_the_google_brain_team_wed_love_to/)\n",
      "\n",
      "[The MalariaSpot Team (2/6/2016)](https://www.reddit.com/r/MachineLearning/comments/4m7ci1/ama_the_malariaspot_team/)\n",
      "\n",
      "[OpenAI Research Team (1/9/2016)](http://www.reddit.com/r/MachineLearning/comments/404r9m/ama_the_openai_research_team/)\n",
      "\n",
      "[Nando de Freitas (12/26/2015)](http://www.reddit.com/r/MachineLearning/comments/3y4zai/ama_nando_de_freitas/)\n",
      "\n",
      "[Andrew Ng and Adam Coates (4/15/2015)](http://www.reddit.com/r/MachineLearning/comments/32ihpe/ama_andrew_ng_and_adam_coates/)\n",
      "\n",
      "[Jürgen Schmidhuber (3/4/2015)](http://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/)\n",
      "\n",
      "[Geoffrey Hinton (11/10/2014)]\n",
      "(http://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/)\n",
      "\n",
      "[Michael Jordan (9/10/2014)](http://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/)\n",
      "\n",
      "[Yann LeCun (5/15/2014)](http://www.reddit.com/r/MachineLearning/comments/25lnbt/ama_yann_lecun/)\n",
      "\n",
      "[Yoshua Bengio (2/27/2014)](http://www.reddit.com/r/MachineLearning/comments/1ysry1/ama_yoshua_bengio/)\n",
      "\n",
      "--------\n",
      "Related Subreddit :\n",
      "\n",
      "* [LearnMachineLearning](http://www.reddit.com/r/LearnMachineLearning)\n",
      "\n",
      "* [Statistics](http://www.reddit.com/r/statistics)\n",
      "\n",
      "* [Computer Vision](http://www.reddit.com/r/computervision)\n",
      "\n",
      "* [Compressive Sensing](http://www.reddit.com/r/CompressiveSensing/)\n",
      "\n",
      "* [NLP] (http://www.reddit.com/r/LanguageTechnology)\n",
      "\n",
      "* [ML Questions] (http://www.reddit.com/r/MLQuestions)\n",
      "\n",
      "* /r/MLjobs and /r/BigDataJobs\n",
      "\n",
      "* /r/datacleaning\n",
      "\n",
      "* /r/DataScience\n",
      "\n",
      "* /r/scientificresearch\n",
      "\n",
      "* /r/artificial\n"
     ]
    }
   ],
   "source": [
    "print(ml_subreddit.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1SmnILuUP5YI"
   },
   "source": [
    "Why doesn't the next cell produce output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iuWtNa0VP5YJ"
   },
   "outputs": [],
   "source": [
    "for submission in submissions:\n",
    "    print(submission.comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'praw.models.listing.generator.ListingGenerator'>\n"
     ]
    }
   ],
   "source": [
    "print(type(submissions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "30k_ILwpP5YL",
    "outputId": "a191c3a0-8e4c-4093-9c9a-dc561d0b6b06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<praw.models.listing.generator.ListingGenerator at 0x7f7f18a99cd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ANSWER:\n",
    "# The API is lazy, and submissions is a generator -- not a data structure:\n",
    "submissions\n",
    "# it must be refreshed in the same cell that invokes its output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "azvexiDQP5YO"
   },
   "source": [
    "Print two comments associated with each of these submissions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nS8skdAfP5YP",
    "outputId": "e6427d89-0f94-4e3c-ce1d-356397e5edae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How long did it take you to ACTUALLY understand python?\n",
      "Where can I find source code for the built-in function `pow()` ?\n",
      "\n",
      "I want to find out why `pow(a, b, 10)` is much faster than `a**b % 10` when you input arbitrarily huge numbers.\n",
      "Awesome job OP. Your job is very much like mine and yes chunk size is a godsend. \n",
      "\n",
      "I wrote a script that read data from a marketing platform API and scanned hundreds of emails for certain terms as we were sun setting some things and looking to consolidate other stuff. Saved the team hundreds of hours of mindless reading to identify terms. LOL\n",
      "\n",
      "GREAT job dude/dudette!\n",
      "I get the feeling this surpasses r/learnpython but I get that from a lot of other posts too.\n",
      "I'll give this book a read when I can. Thank you!\n",
      "I haven't read it yet, but it looks really nice. Thank you very much, for giving this away for free.\n",
      "one way is, instead of using `flask run` use `python app.py` and then your `app.run` settings will be used. \n",
      "\n",
      "another way is, `flask run --ip=0.0.0.0 --port=5000`\n",
      "Most of these ecommerce sites use javascript for dynamic updating the content. Look into Selenium instead https://automatetheboringstuff.com/2e/chapter12/ (second part)\n",
      "Before to scrape a website, consult its [robots.txt](https://www.amazon.fr/robots.txt) file.\n",
      "This file gives you the list of subfolders you can or you can't scrape.\n",
      "If the page you need to scrape is preceding by 'disallow', you need a other way to do. \n",
      "Selenium should be the library you need, it simulate a webbrowser.\n",
      "not sure but there's: https://jupyterbook.org/publish/gh-pages.html\n",
      "\n",
      "also if you can just script your manual deploy process, then you could schedule that script to run daily\n",
      "Using regular expressions:\n",
      "\n",
      "    import re\n",
      "    pattern = \"[0-9]+\"\n",
      "    text = \"Hello29, this is some 2939 text 0\"\n",
      "    result = re.findall(pattern, text)\n",
      "    print(result)\n",
      "    >>> ['29', '2939', '0']\n"
     ]
    }
   ],
   "source": [
    "submissions = reddit.subreddit('learnpython').hot(limit=10)\n",
    "for submission in submissions:\n",
    "    top_level_comments = list(submission.comments)\n",
    "    all_comments = submission.comments.list()[:2]\n",
    "    for comment in all_comments:\n",
    "        print(comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mpd_xVbGP5YS"
   },
   "source": [
    "Referring to the API documentation, explore the submissions object and print some interesting data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gJmNGjdGP5YU"
   },
   "outputs": [],
   "source": [
    "# submissions = reddit.subreddit('learnpython').banned\n",
    "# submissions = reddit.subreddit('learnpython').collections\n",
    "# submissions = reddit.subreddit('learnpython').comments\n",
    "# submissions = reddit.subreddit('learnpython').contributor\n",
    "# submissions = reddit.subreddit('learnpython').controversial\n",
    "# submissions = reddit.subreddit('learnpython').display_name\n",
    "# submissions = reddit.subreddit('learnpython').emoji\n",
    "# submissions = reddit.subreddit('learnpython').filters\n",
    "# submissions = reddit.subreddit('learnpython').flair\n",
    "# submissions = reddit.subreddit('learnpython').fullname\n",
    "# submissions = reddit.subreddit('learnpython').gilded\n",
    "# submissions = reddit.subreddit('learnpython').hot\n",
    "# submissions = reddit.subreddit('learnpython').message\n",
    "# submissions = reddit.subreddit('learnpython').MESSAGE_PREFIX\n",
    "# submissions = reddit.subreddit('learnpython').mod\n",
    "# submissions = reddit.subreddit('learnpython').moderator\n",
    "# submissions = reddit.subreddit('learnpython').mutedodmail\n",
    "# submissions = reddit.subreddit('learnpython').muted\n",
    "# submissions = reddit.subreddit('learnpython').new\n",
    "# submissions = reddit.subreddit('learnpython').parse\n",
    "# submissions = reddit.subreddit('learnpython').post_requirements\n",
    "# submissions = reddit.subreddit('learnpython').quaran\n",
    "# submissions = reddit.subreddit('learnpython').random\n",
    "# submissions = reddit.subreddit('learnpython').random_rising\n",
    "# submissions = reddit.subreddit('learnpython').rules\n",
    "# submissions = reddit.subreddit('learnpython').search\n",
    "# submissions = reddit.subreddit('learnpython').sticky\n",
    "# submissions = reddit.subreddit('learnpython').stream\n",
    "# submissions = reddit.subreddit('learnpython').stylesheet\n",
    "# submissions = reddit.subreddit('learnpython').submit\n",
    "# submissions = reddit.subreddit('learnpython').submit_image\n",
    "# submissions = reddit.subreddit('learnpython').submit_poll\n",
    "# submissions = reddit.subreddit('learnpython').submit_video\n",
    "# submissions = reddit.subreddit('learnpython').subscribe\n",
    "# submissions = reddit.subreddit('learnpython').top\n",
    "# submissions = reddit.subreddit('learnpython').traffic\n",
    "# submissions = reddit.subreddit('learnpython').unsubscribe\n",
    "# submissions = reddit.subreddit('learnpython').widgets\n",
    "# submissions = reddit.subreddit('learnpython').wiki\n",
    "\n",
    "submissions = reddit.subreddit('learnpython').\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "km8Ujm42P5YW"
   },
   "source": [
    "#### Posting to Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2WNJySAzP5YX"
   },
   "source": [
    "To be able to post to your Reddit account (i.e. contribute submissions), you need to connect to the API with read/write privilege. This requires an *authorised instance*, which is obtained by including your Reddit user name and password in the connection request: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "leC2dfG2P5YY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id='my client id',\n",
    "                     client_secret='my client secret',\n",
    "                     user_agent='my user agent',\n",
    "                     username='my username',\n",
    "                     password='my password')\n",
    "print(reddit.read_only)  # Output: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwnCOxWJP5YZ"
   },
   "source": [
    "You could hide these last two credentials by adding them to your JSON file and then reading all five values at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_auth = 'auth_reddit.json'\n",
    "auth = json.loads(open(path_auth).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MUvzVglbP5YZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id='my client id',\n",
    "                     client_secret='my client secret',\n",
    "                     user_agent='my user agent',\n",
    "                     username='my username',\n",
    "                     password='my password')\n",
    "print(reddit.read_only)  # Output: False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fWOk43cgN71c"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > © 2021 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DSIA Lab 2.2.3 - ANSWER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
